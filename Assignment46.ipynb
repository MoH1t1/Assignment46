{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n\nR-squared measures the proportion of variance in the dependent variable (Y) explained by the independent variable(s).\n\nCalculation: R^2 = 1- (SSR/SST) \n\nWhere \nSSR = Residual Sum of Square\nSST = Total Sum of Squares.\nRepresents: How well the model fits the data (e.g., R^2 =0.8 means 80% of variance in  Y is explained).\n\n# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n\nAdjusted R-squared is a modified version of R^2 that adjusts for the number of predictors in the model. It penalizes the addition of predictors that do not significantly improve \nthe model's explanatory power.\n\nDifference:\n\nDefinition:\nR^2: Proportion of variance in the dependent variable explained by the model.\nAdjusted R^2 : Adjusted version of R^2 that accounts for the number of predictors.\n                                                    \nEffect of Adding Predictors:\nR^2: Always increases or stays the same when a new predictor is added, regardless of its significance.\nAdjusted R^2 : Decreases if the new predictor does not improve the model significantly.\n\nOverfitting:\nR^2: Does not penalize overfitting caused by adding unnecessary predictors.\nAdjusted R^2: Penalizes the addition of irrelevant predictors, reducing overfitting risk.\n\n# Q3. When is it more appropriate to use adjusted R-squared?\n\nUse when comparing models with different numbers of predictors to avoid overfitting.\n\n# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n                                                                                                                 ^\nMean Squared Error (MSE): MSE measures the average of the squared differences between actual (yi) and predicted (yi) values.\nFormula:\n                      n        ^\n                MSE = ‚àë (yùëñ - yùëñ)^2 /n\n                     i=1\nRepresents: Penalizes large errors more than small errors due to squaring. A smaller MSE indicates a better model.\n\nRoot Mean Squared Error (RMSE): RMSE is the square root of MSE, providing error in the same units as the target variable.\nFormula:\n                  n       ^\nRMSE = root over {‚àë |yùëñ - yùëñ|^2 / n}\n                 i=1\n\nRepresents: Reflects the magnitude of prediction errors and is sensitive to large errors.\n\nMean Absolute Error (MAE): MAE measures the average of the absolute differences between actual and predicted values.\nFormula:\n         n       ^\nRMSE =  {‚àë |yùëñ - yùëñ| / n}\n        i=1\n\nRepresents: Indicates the average error magnitude and is less sensitive to outliers compared to RMSE/MSE.\n\n# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n\n1. Mean Squared Error (MSE) \n\nAdvantages:  \n- Penalizes large errors more, making it sensitive to outliers.  \n- Smooth and differentiable, suitable for optimization algorithms.  \n- Widely used in machine learning.  \n\nDisadvantages:  \n- Difficult to interpret due to squared units.  \n- Overly emphasizes large errors, which may not always be desirable.  \n\n\n2. Root Mean Squared Error (RMSE)  \n\nAdvantages:  \n- Errors are in the same units as the target variable, improving interpretability.  \n- Emphasizes large errors, making it suitable for applications where large deviations matter.  \n- Commonly used in real-world applications for its clarity.  \n\nDisadvantages:  \n- Sensitive to outliers like MSE.  \n- More computationally expensive due to the square root operation.  \n\n3. Mean Absolute Error (MAE)\n\nAdvantages:  \n- Easy to interpret as it measures average error magnitude.  \n- Robust to outliers compared to MSE and RMSE.  \n- Reflects actual prediction error without squaring or taking roots.  \n\nDisadvantages:  \n- Does not penalize large errors as strongly as MSE or RMSE.  \n- Less smooth and differentiable, which can complicate optimization.  \n\n\n# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n\nLasso Regularization (Least Absolute Shrinkage and Selection Operator): Lasso is a type of regularization technique used in linear regression models to prevent overfitting \nby adding a penalty to the magnitude of the regression coefficients.\n\nLasso vs. Ridge: While both Lasso and Ridge regularization aim to shrink the coefficients, Lasso tends to set some coefficients to exactly zero (performing feature selection), \nwhereas Ridge only shrinks the coefficients but does not eliminate any feature completely. Hence, Lasso is often preferred when feature selection is required, while Ridge is more \nappropriate when we expect all features to contribute, but want to regularize their effects.\n\nUses: \nWhen we have a high-dimensional dataset with many features.\nwhen we suspect that some features are irrelevant or redundant and want to automatically perform feature selection by shrinking some coefficients to zero.\n\n# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n\nRegularized models, like Lasso and Ridge, add a penalty term to the loss function, reducing the size of the coefficients and preventing overfitting by simplifying the model.\nThis helps the model generalize better to unseen data.\n\nExample: A linear regression model with many features might overfit the data. Regularization shrinks coefficients of less important features, reducing overfitting and improving \ntest performance.\n\n# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n\nLimitations:\nLinear Assumption: They assume a linear relationship, which might not work well for complex, nonlinear data.\nFeature Selection: Lasso can overly simplify the model, removing too many features.\nChoosing Œª: The regularization strength needs careful tuning; improper choices can lead to underfitting or overfitting.\nMulticollinearity: Regularized models might still struggle with highly correlated features.\nNon-additive Effects: They may not capture feature interactions well.\n    \nUse: If the data has nonlinear relationships or interactions between features, nonlinear models (e.g., decision trees or neural networks) might be better.\n\n# Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you \n    # choose as the better performer, and why? Are there any limitations to your choice of metric?\n\nRMSE (Root Mean Squared Error) measures the square root of the average squared differences between predicted and actual values. It penalizes larger errors more heavily due to the \nsquaring of differences.\nMAE (Mean Absolute Error) calculates the average of the absolute differences between predicted and actual values. It treats all errors equally.\n    \nModel Comparison:\nModel A has an RMSE of 10, which means it has some larger errors, as RMSE is sensitive to outliers.\nModel B has an MAE of 8, indicating it has relatively smaller errors on average, with no penalty for outliers.\n                                                                                                \nWhich Model to Choose:\n\nIf we care more about large errors and want to penalize them more (e.g., in applications where large errors are costly), we might prefer Model A (lower RMSE).\nIf we prefer a model that treats all errors equally and doesn't penalize outliers as harshly, Model B (lower MAE) might be a better choice.\n\nLimitations:\nRMSE is more sensitive to outliers, which may not always be desirable.\nMAE does not give much weight to large errors, so it may be less sensitive to models that make significant mistakes on certain predictions.\n\n# Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter \n# of 0.1, while Model Buses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n\nRidge (L2 regularization) adds a penalty proportional to the sum of the squared coefficients, encouraging smaller coefficients without necessarily setting any of them to zero.\nLasso (L1 regularization) adds a penalty proportional to the sum of the absolute values of the coefficients, which can set some coefficients to zero, effectively performing feature selection.\n                                             \nModel Comparison:\nModel A (Ridge, Œª=0.1): Ridge regularization would help in shrinking the coefficients but does not eliminate any features. It is useful when all features are expected to have some effect on the target variable.\nModel B (Lasso, Œª=0.5): Lasso regularization can eliminate some features by setting their coefficients to zero, which is useful for feature selection, especially if you believe some features are irrelevant.\n                                                                                                                                                     \nWhich Model to Choose:\nChoose Model A (Ridge) if you believe that all features should contribute to the model, and you are only interested in reducing the magnitude of the coefficients to prevent overfitting.\nChoose Model B (Lasso) if you expect many features to be irrelevant and you want to perform feature selection automatically by shrinking some coefficients to zero.\n                                                                                                                                                     \nTrade-offs and Limitations:\nRidge: Works well when all features are potentially useful, but it cannot perform feature selection, so the model may remain complex.\nLasso: Great for feature selection but may discard useful features, especially when there are correlated features.\nThe choice of Œª impacts model performance: a small Œª may result in overfitting, while a large Œª could lead to underfitting. Cross-validation is essential to tune Œª.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}